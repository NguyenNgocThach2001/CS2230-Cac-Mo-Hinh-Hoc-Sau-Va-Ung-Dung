Thầy Tiệp (Sẽ bắt buộc đặt câu hỏi). Được note theo thời gian từ đầu tới cuối buổi học. 
Lưu ý note này không nói lên chất lượng lớp học và giảng viên. Thầy giảng rất chi tiết, nhiều thứ khác không được note trong này. Đây chỉ là tóm tắt học viên note lại.

Tỉ lệ điểm của môn:
Bài tập: 3 (Bài tập sẽ nộp lên hệ thống, có chấm điểm tự động, không quá khó).
Đồ án nhóm: 3 (Sẽ cung cấp danh sách đề tài).
Thi cuối kỳ: 4 
  - Thi viết.
  - Lý thuyết: ví dụ: overfitting là gì, gradient vanishing là gì.
  - Coding: code trên giấy, đảm bảo làm trong phần bài tập.
  - Ứng dụng: ví dụ: cho trường hợp thực tế, trình bày cách xây dựng mô hình phù hợp dựa trên trường hợp đó.
	+ Phân tích bài toán.
	+ Dữ liệu lấy ở đâu, chuẩn bị thế nào?
	+ Chọn mô hình gì cho phù hợp?
	+ Đánh giá kết quả đã train như thế nào?

Sách thầy gợi ý: 
- Deep learning  (của tác giả vừa nhận giải Vin gì đó).

Môn học dùng python.

-----------------------------------------------------------------------------------------------------------------------------------
Các nền tảng cần có khi học môn này:
Toán: 
- Đại số tuyến tính, + - * / ma trận. Vì tất cả dữ liệu đều được biểu diễn dưới dạng vector hoặc ma trận (ảnh, text, âm thanh).
- Giải tích: đạo hàm. Vì các mô hình máy học mình sẽ chủ yếu giải quyết bài toán tối ưu => cần học đạo hàm.
Shallow models cần coi lại: 
- Linear Reg 
- Logistic Reg
- SoftMax Reg
...
-------------------------------------------------------------------------------------------------------------------------------------

Các quy mô dữ liệu hiện tại:
  Data set  
  Large - scale dataset (Môn này đầu ra chỉ ở mức độ này)
  Internet scale dataset. (Môn này ở đây thì chỉ finetune, chỉ train các ngạch hẹp).

-----------------------------------------------------------------------
Nội dung chính môn sẽ học: 
Ảnh: 
- CNN
- ViT

Văn bản: 
- RNN
- Attention
- Transformer BERT, GPT, TS
-----------------------------------------------------------------------


----------------------------------------------------------------------
Các câu hỏi trong lớp học (từ học viên):
CH1: Làm sao tránh overfitting? Dùng nhiều độ đo khác nhau để đánh giá mô hình. Hoặc Early Stopping. Xử lý dữ liệu (noise)?
=>
2 hướng hay sử dụng:
1. Tăng thêm kích thước dữ liệu.
2. Giảm độ phức tạp mô hình.
CH2: Dùng API của các cty lớn hay tự host model?
=> thông thường thích dùng API hơn vì đơn giản, ít chi phí, hiệu quả.
Còn nhiều lắm note không nổi
----------------------------------------------------------------------

-----------------------------------------------------------------------
Nhìn ván đề:
- Problem -> why -> solution. 
- cần di chuyển từ A -> B mua nước suối, chọn đi bộ, mỏi chân -> dùng sức của mình nhiều -> đi xe ngựa.
- đi xe ngựa từ A -> B, ngựa mỏi chân -> dùng sức ngựa nhiều -> đi xe máy.
- đi xe máy từ A -> B sang tỉnh khác, không an toàn -> vì mưa, bụi, nắng, ... -> đi ô tô.
- đi ô tô từ A -> B sang nước, không di chuyển được qua địa hình phức tạp như núi, sông, biển, ... -> Vì xe phải bám vào mặt đường chạy được -> đi máy bay.

------------------------------------------------------------------------


Nền tảng đại số tuyến tính:
- Scalar, vector, matrix, tensor.
	+ Scalar: những giá trị vô hướng, ký hiệu viết thường.
	+ Vector: biểu diễn dữ liệu có nhiều chiều gồm: vận tốc, lực, điểm, ... ký hiệu viết thường và in đậm. vector mặc định sẽ ở dạng cột.
	+ Matrix: gồm nhiều vector, ký hiệu viết hoa.
	+ Tensor: A thuộc Rmxnxp. gồm nhiều hần tử được bố trí vào một không gian nhiều hơn 2 chiều. Ký hiệu viết hoa in đậm.
- Tổng 2 vector.
- Nhân 2 vector.
- Tích hađâm.
- Tích vô hướng 2 vector.
Ví dụ minh họa nhân 2 vector:
- Cho 2 vector x^T và y. (x^T ngang, y dọc).
- Vector x biểu hiện số tín chỉ của các môn học.
- Vector y biểu hiện kết quả thi của môn học.
- Vector kết quả là tổng số tín chỉ * điểm thi
x^T.y = [3 3 4 2 1] . |9| = [ a ] 
		      |6|
		      |5|
		      |8|
		      |1|
Ví dụ chuyển vị của ma trận:
|1 2|^T
|4 5|  = | 1 4 6 |
|6 7|	 | 2 5 7 |

--------------------------------------------------------------------------
Nền tảng giải tích:
- Học lại khái niệm đạo hàm, các công thức đạo hàm cơ bản nhất.
- Đạo hàm thầy Tiệp thích nhất là đạo hàm của e^x. Vì không làm gì cả, người ta thường thiết kế các mô hình có chứa các công thức dễ tính đạo hàm. Ví dụ hàm sigmoid(x) = 1 / (1+e^-x)

Công thức cần lưu ý (đạo hàm hàm hợp):
[f(g(x))]' = f'(g(x)) * g'(x)
[f1(f2(f3(...fn(x)]' = (df1/df2) * (df2/df3) * ... * (dfn-1/dfn) * (dfn/x). (Chain rule)
Cái này có thể gây ra vanishing gradient vì các cụm df/df cho giá trị rất nhỏ và kết quả = 0. 

Đạo hàm riêng (partial dẻivatives) của hàm nhiều biến theo biến x.
Gradient. 

delta y/ delta x => đạo hàm xấp xỉ.
dy/dx => đạo hàm 1 biến
∂y/∂x => đạo hàm của hàm đa biến theo 1 biến
(tam giac ngược) nabla x y => đạo hàm theo vector.

--------------------------------------------------------------------------

Tất cả các mô hình máy học:
x 
f theta(x) model
y~ kết quả dự đoán
y  kết quả thực tế
Loss(theta): hàm để y~ cho ra kết quả gần y nhất.
 ________________________
|x -> ftheta(x) -> y~ ≈ y|

Công việc của 1 nhà khoa học deep learning:
- Thiết kế ftheota(x)
- Thiết kế hàm Loss(theta)
- Tìm theta* để Loss(theta) nhỏ nhất.

---------------------------------------------------------------------------
Gradient là gì?
1. Quan sát hàm số 1 biến:
Ở cấp 3, mình hay dùng đạo hàm để quan sát sự thay đổi của hàm số đúng không?
- Ví dụ: 
hàm y = f(x) = 2x^2
đạo hàm của hàm f(x) là hàm f'(x) = 4x .(hàm này có ý nghĩa gì?)
Giả sử cần QUAN SÁT hàm f(x) tại điểm x = 2. 
Thế 2 vào hàm f'(x) ta có f'(2) = 8. Con số 8 này biểu thị ĐỘ DỐC của hàm f(x) tại điểm x = 2.
Độ dốc này dương, tức là tại điểm x = 2, trục y đang đi lên với tốc độ 8. (chỉ xét trên 1 khoảng tăng rất nhỏ) 
Ngược lại trục y đang giảm nếu ra con số -8.
Tại sao đạo hàm, độ dốc cho biết đồ thị đang tăng hay giảm? Nói về độ dốc, người ta thường tính độ dốc giữa 2 điểm,
cứ nghĩ tới ô tô đang lên dốc, bạn chọn 2 điểm là điểm đầu dốc (x1;y1) và điểm cuối dốc (x2;y2). Bạn tính
delta x = x2-x1 và delta y = y2-y1
rồi tính slope = delta y/delta x. Ngẫm thử xem phép tính trên có biểu thị được độ dốc không? :D
Vậy thế nào là độ dốc tại 1 điểm? Khi 2 điểm RẤT GẦN NHAU, độ dốc giữa 2 điểm đó có thể xem là độ dốc tại 1 điểm.

2. Quan sát hàm số nhiều biến:
Ở trên chỉ đề cập hàm số phụ thuộc vào 1 biến x. Vậy hàm số phụ thuộc vào 2 biến x,y thì sao? Tính độ dốc kiểu gì?
- Ví dụ:
hàm z = f(x,y) = 2x^2 + y^2
Bây giờ có 2 biến thì đạo hàm sao? Quan sát kiểu gì? Độ dốc tại 2 điểm tính sao?
Giải pháp là đạo hàm theo từng biến, cứ mặc kệ các biến khác, quan sát sự thay đổi của 1 biến 1 lúc thôi.
Đạo hàm của f(x,y) theo x là 4x.
Đạo hàm của f(x,y) theo y là 2y.
Giả sử cần quan sát hàm f(x,y) tại điểm x=2,y=-5.
x = 2 => 4*x = 4*2 = 8. Vậy với x=2, dương, tại điểm x=2 khi x tăng thì z sẽ tăng (chỉ xét trên 1 khoảng tăng rất nhỏ) [quan sát 1]
y = -5 => 2*y = 2*-5 = -10. Vậy với y=-5, âm, tại điểm y=-5 khi y tăng thì z sẽ giảm (chỉ xét trên 1 khoảng giảm rất nhỏ) [quan sát 2]
dựa trên [quan sát 1] và [quan sát 2], có phải bạn đã biết tại điểm x=2 và y=-5 thì đồ thị có xu hướng thay đổi thế nào rồi đúng không?
Vậy gradient là gì? Là vector chứa tất cả các đạo hàm riêng của hàm f(x,y).
Tức gradient là vector [4x,2y]. Có thể thêm các biến z k i l, gradient có thể là vector [4x,2y,3z,5ln(k),1.5i,-5l^2]

---------------------------------------------------------------------------
1. Khi bạn biết độ dốc của hàm số đang âm hay dương, bạn có tìm được hướng đi để đi đến điểm có giá trị thấp nhất không?
Mình có 1 nhận xét thế này:
- Nếu tại điểm x có độ dốc âm, tức y đang đi xuống => tôi cứ tăng x để y đi xuống.
- Nếu tại điểm x có độ dốc dương, tức y đang đi lên => tôi phải giảm x để y đi xuống.
Tăng và giảm x với giá trị bao nhiêu? Cái này mình không chứng minh được, cách đơn giản nhất mà mọi người hay dùng là đi 1 khoảng - learning rate * gradient. Tức x mới = x - learning rate * gradietn.

2. Vậy với hàm nhiều biến, đi thế nào?
- Ví dụ có hàm f(x,y,z). Giả sử tính được gradient là vector [2x, -5y, 10z^2].  
di chuyển x: 
	x = x - learning rate * độ dốc x (2x)
di chuyển y:
	y = y - learning rate * độ dốc y (-5y)
di chuyển z: 
	z = z - learning rate * độ dốc z (10z^2)
=> nói ngắn gọi là cứ đi ngược gradient. 
Cách ở trên gọi là gradient descent. descent là cứ đi làm sao cho nó giảm đấy.